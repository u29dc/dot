# Codex Global Configuration
# See https://github.com/openai/codex/blob/main/docs/config.md

# ------------------------------------------------------------------------------
# CORE DEFAULTS
# ------------------------------------------------------------------------------

model = "gpt-5-codex"
model_provider = "openai"                 # Default to OpenAI provider
model_reasoning_effort = "high"
model_reasoning_summary = "detailed"      # Verbose reasoning output (auto|concise|detailed|none)
model_verbosity = "high"                  # Maximum output detail (low|medium|high)
sandbox_mode = "workspace-write"          # Allow file operations in workspace
approval_policy = "on-request"            # Ask before dangerous operations (untrusted|on-failure|on-request|never)

[sandbox_workspace_write]
    network_access = true

# ------------------------------------------------------------------------------
# MODEL PROVIDERS
# ------------------------------------------------------------------------------

[model_providers.openai]
name = "OpenAI"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
wire_api = "chat"

[model_providers.ollama]
name = "Ollama Local"
base_url = "http://localhost:11434/v1"
# No API key required for local Ollama server

[model_providers.openrouter]
name = "OpenRouter"
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"
wire_api = "chat"

# ------------------------------------------------------------------------------
# PROFILES
# Use with: codex -p <profile_name> "your prompt"
# ------------------------------------------------------------------------------

# codex -p fast
[profiles.fast]
model = "gpt-5-codex"
model_provider = "openai"
model_reasoning_effort = "low"
model_reasoning_summary = "concise"
model_verbosity = "low"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# codex -p research
[profiles.research]
model = "gpt-5-codex"
model_provider = "openai"
model_reasoning_effort = "high"
model_reasoning_summary = "detailed"
model_verbosity = "high"
approval_policy = "never"
sandbox_mode = "read-only"

# codex -p router
[profiles.router]
model = "anthropic/claude-sonnet-4.5"
model_provider = "openrouter"
model_reasoning_effort = "high"
model_reasoning_summary = "detailed"
model_verbosity = "high"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# codex -p local
[profiles.local]
model = "qwen3:latest"
model_provider = "ollama"
model_reasoning_effort = "medium"
model_verbosity = "high"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# ------------------------------------------------------------------------------
# TRUSTED PROJECTS
# Projects with elevated trust levels for reduced approval prompts
# ------------------------------------------------------------------------------

[projects."~/Git"]
trust_level = "trusted"

# ------------------------------------------------------------------------------
# MCP SERVERS
# ------------------------------------------------------------------------------

[mcp_servers.svelte]
command = "bunx"
args = ["@sveltejs/mcp"]
